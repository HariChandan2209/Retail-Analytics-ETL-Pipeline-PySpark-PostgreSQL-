# Retail Analytics ETL Pipeline (PySpark â†’ PostgreSQL)

## ðŸ“š Project Description

This project builds an **end-to-end ETL pipeline** using **PySpark** to extract, clean, transform, and load complex retail transactional data into a **PostgreSQL data warehouse**.

The pipeline handles multi-entity datasets (Customers, Products, Orders, Order Items) and creates a normalized warehouse model suitable for business intelligence and advanced analytics.

---

## ðŸš€ Project Structure

```plaintext
retail_etl_project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ customers.csv
â”‚   â”œâ”€â”€ products.csv
â”‚   â”œâ”€â”€ orders.csv
â”‚   â””â”€â”€ order_items.csv
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â”œâ”€â”€ load.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ generate_data.py
â”œâ”€â”€ run_pipeline.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
